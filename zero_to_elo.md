- Game of Go
  - Go settings, rules, state space and action space
- History of go program and their methods
  - CrazyStone  (Only this one)
  - Darkforest
  - ZenGo
- Problem setting
  - Adversarial, Deterministic, Fully observable
    - Adversarial: someone wants to kill you 
    - Deterministic: because you can be sure that the stone is where you actually place it
    - Fully observable: this debatable but in principle you just need the board picture and the rule to play
- Reinforcement (How to learn from this signal)
  - RL setting 
  - RL objectives V, Q
    - Notice that those are expections and we could use Monte Carlo to evaluate
  - Why simple q-learning doesn't work?
    - State space is large
    - No clear rw signal and very sparse
    - Thus exploration of the space is difficult for a "contraction kind of method"
    - No assumption on the goodness of the environment (friendly or not)
- Monte Carlo search tree (How to generate a learning signals)
  - Where MCTS comes from? 
  - How does it work?
  - MCTS as in Cesar
- Bandits (How to select the best action)
  - what are bandits arm poblems?
  - regret analysis
  - UCB1
  - Context-aware regret
- Self play
- A primer on deep learning
- Code demo

